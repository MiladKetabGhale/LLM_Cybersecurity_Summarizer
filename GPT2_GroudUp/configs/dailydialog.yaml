tokenizer:
  name: gpt2

data:
  dataset_name: daily_dialog
  max_length: 128
  stride: 128
  batch_size: 4

model:
  emb_dim: 128
  context_length: 128
  n_heads: 4
  n_layers: 6
  drop_rate: 0.1

train:
  learning_rate: 0.0003
  num_epochs: 3
  model_output_path: GroundUp_ModelTraining_Outcome/gpt2_dailydialog.pt
