## üìö Resources

This project was developed with inspiration and learning from the following resources:

### üìù Papers

- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)  
  Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen.

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)  
  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin.

- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)  
  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.

---

### üìñ Books

- [Natural Language Processing with Transformers](https://transformersbook.com/)  
- [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)  
  Sebastian Raschka.
- [High Performance Python (2nd Edition)](https://www.oreilly.com/library/view/high-performance-python/9781492055013/)  
  Micha Gorelick, Ian Ozsvald.

---

### üé• YouTube Channels

- [Let‚Äôs build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY&list=WL&index=36)  
  Andrej Karpathy.

