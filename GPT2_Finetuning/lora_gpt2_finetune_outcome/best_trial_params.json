{
  "learning_rate": 0.0003990311106332582,
  "weight_decay": 0.05,
  "num_train_epochs": 5,
  "per_device_train_batch_size": 4,
  "gradient_accumulation_steps": 2,
  "lora_r": 32,
  "lora_alpha": 96
}